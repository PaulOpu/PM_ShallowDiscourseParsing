{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "from conll16st.confusion_matrix import ConfusionMatrix, Alphabet\n",
    "from conll16st.conn_head_mapper import ConnHeadMapper\n",
    "import conll16st.validator as validator\n",
    "\n",
    "CONN_HEAD_MAPPER = ConnHeadMapper()\n",
    "\n",
    "def evaluate(gold_list, predicted_list):\n",
    "    connective_cm = evaluate_connectives(gold_list, predicted_list)\n",
    "    arg1_cm, arg2_cm, rel_arg_cm = evaluate_argument_extractor(gold_list, predicted_list)\n",
    "    sense_cm = evaluate_sense(gold_list, predicted_list)\n",
    "\n",
    "    print 'Explicit connectives         : Precision %1.4f Recall %1.4f F1 %1.4f' % connective_cm.get_prf('yes')\n",
    "    print 'Arg 1 extractor              : Precision %1.4f Recall %1.4f F1 %1.4f' % arg1_cm.get_prf('yes')\n",
    "    print 'Arg 2 extractor              : Precision %1.4f Recall %1.4f F1 %1.4f' % arg2_cm.get_prf('yes')\n",
    "    print 'Arg1 Arg2 extractor combined : Precision %1.4f Recall %1.4f F1 %1.4f' % rel_arg_cm.get_prf('yes')\n",
    "    print 'Sense classification--------------'\n",
    "    sense_cm.print_summary()\n",
    "    print 'Overall parser performance --------------'\n",
    "    precision, recall, f1 = sense_cm.compute_micro_average_f1()\n",
    "    print 'Precision %1.4f Recall %1.4f F1 %1.4f' % (precision, recall, f1)\n",
    "    return connective_cm, arg1_cm, arg2_cm, rel_arg_cm, sense_cm, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_argument_extractor(gold_list, predicted_list):\n",
    "    \"\"\"Evaluate argument extractor at Arg1, Arg2, and relation level\n",
    "\n",
    "    \"\"\"\n",
    "    gold_arg1 = [(x['DocID'], x['Arg1']['TokenList']) for x in gold_list]\n",
    "    predicted_arg1 = [(x['DocID'], x['Arg1']['TokenList']) for x in predicted_list]\n",
    "    arg1_cm = compute_binary_eval_metric(gold_arg1, predicted_arg1, span_exact_matching)\n",
    "\n",
    "    gold_arg2 = [(x['DocID'], x['Arg2']['TokenList']) for x in gold_list]\n",
    "    predicted_arg2 = [(x['DocID'], x['Arg2']['TokenList']) for x in predicted_list]\n",
    "    arg2_cm = compute_binary_eval_metric(gold_arg2, predicted_arg2, span_exact_matching)\n",
    "\n",
    "    gold_arg12 = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList'])) \\\n",
    "            for x in gold_list]\n",
    "    predicted_arg12 = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList'])) \\\n",
    "            for x in predicted_list]\n",
    "    rel_arg_cm = compute_binary_eval_metric(gold_arg12, predicted_arg12, spans_exact_matching)\n",
    "    return arg1_cm, arg2_cm, rel_arg_cm\n",
    "\n",
    "def evaluate_connectives(gold_list, predicted_list):\n",
    "    \"\"\"Evaluate connective recognition accuracy for explicit discourse relations\n",
    "\n",
    "    \"\"\"\n",
    "    explicit_gold_list = [(x['DocID'], x['Connective']['TokenList'], x['Connective']['RawText']) \\\n",
    "            for x in gold_list if x['Type'] == 'Explicit']\n",
    "    explicit_predicted_list = [(x['DocID'], x['Connective']['TokenList']) \\\n",
    "            for x in predicted_list if x['Type'] == 'Explicit']\n",
    "    connective_cm = compute_binary_eval_metric(\n",
    "            explicit_gold_list, explicit_predicted_list, connective_head_matching)    \n",
    "    return connective_cm\n",
    "\n",
    "def spans_exact_matching(gold_doc_id_spans, predicted_doc_id_spans):\n",
    "    \"\"\"Matching two lists of spans\n",
    "\n",
    "    Input:\n",
    "        gold_doc_id_spans : (DocID , a list of lists of tuples of token addresses)\n",
    "        predicted_doc_id_spans : (DocID , a list of lists of token indices)\n",
    "\n",
    "    Returns:\n",
    "        True if the spans match exactly\n",
    "    \"\"\"\n",
    "    exact_match = True\n",
    "    gold_docID = gold_doc_id_spans[0]\n",
    "    gold_spans = gold_doc_id_spans[1]\n",
    "    predicted_docID = predicted_doc_id_spans[0]\n",
    "    predicted_spans = predicted_doc_id_spans[1]\n",
    "\n",
    "    for gold_span, predicted_span in zip(gold_spans, predicted_spans):\n",
    "        exact_match = span_exact_matching((gold_docID,gold_span), (predicted_docID, predicted_span)) \\\n",
    "                and exact_match\n",
    "    return exact_match\n",
    "\n",
    "def span_exact_matching(gold_span, predicted_span):\n",
    "    \"\"\"Matching two spans\n",
    "\n",
    "    Input:\n",
    "        gold_span : a list of tuples :(DocID, list of tuples of token addresses)\n",
    "        predicted_span : a list of tuples :(DocID, list of token indices)\n",
    "\n",
    "    Returns:\n",
    "        True if the spans match exactly\n",
    "    \"\"\"\n",
    "    gold_docID = gold_span[0]\n",
    "    predicted_docID = predicted_span[0]\n",
    "    if gold_docID != predicted_docID:\n",
    "        return False\n",
    "    gold_token_indices = [x[2] for x in gold_span[1]]\n",
    "    predicted_token_indices = predicted_span[1]\n",
    "    return gold_docID == predicted_docID and gold_token_indices == predicted_token_indices\n",
    "\n",
    "def connective_head_matching(gold_raw_connective, predicted_raw_connective):\n",
    "    \"\"\"Matching connectives\n",
    "\n",
    "    Input:\n",
    "        gold_raw_connective : (DocID, a list of tuples of token addresses, raw connective token)\n",
    "        predicted_raw_connective : (DocID, a list of tuples of token addresses)\n",
    "\n",
    "    A predicted raw connective is considered iff\n",
    "        1) the predicted raw connective includes the connective \"head\"\n",
    "        2) the predicted raw connective tokens are the subset of predicted raw connective tokens\n",
    "\n",
    "    For example:\n",
    "        connective_head_matching('two weeks after', 'weeks after')  --> True\n",
    "        connective_head_matching('two weeks after', 'two weeks')  --> False not covering head\n",
    "        connective_head_matching('just because', 'because')  --> True\n",
    "        connective_head_matching('just because', 'simply because')  --> False not subset\n",
    "        connective_head_matching('just because', 'since')  --> False\n",
    "    \"\"\"\n",
    "    gold_docID, gold_token_address_list, gold_tokens = gold_raw_connective\n",
    "    predicted_docID, predicted_token_list = predicted_raw_connective\n",
    "    if gold_docID != predicted_docID:\n",
    "        return False\n",
    "\n",
    "    gold_token_indices = [x[2] for x in gold_token_address_list]\n",
    "\n",
    "    if gold_token_address_list == predicted_token_list:\n",
    "        return True\n",
    "    elif not set(predicted_token_list).issubset(set(gold_token_indices)):\n",
    "        return False\n",
    "    else:\n",
    "        conn_head, indices = CONN_HEAD_MAPPER.map_raw_connective(gold_tokens)\n",
    "        gold_head_connective_indices = [gold_token_indices[x] for x in indices]\n",
    "        return set(gold_head_connective_indices).issubset(set(predicted_token_list))\n",
    "\n",
    "def evaluate_sense(gold_list, predicted_list):\n",
    "    \"\"\"Evaluate sense classifier\n",
    "\n",
    "    The label ConfusionMatrix.NEGATIVE_CLASS is for the relations \n",
    "    that are missed by the system\n",
    "    because the arguments don't match any of the gold relations.\n",
    "    \"\"\"\n",
    "    sense_alphabet = Alphabet()\n",
    "    valid_senses = validator.identify_valid_senses(gold_list)\n",
    "    for relation in gold_list:\n",
    "        sense = relation['Sense'][0]\n",
    "        if sense in valid_senses:\n",
    "            sense_alphabet.add(sense)\n",
    "\n",
    "    sense_alphabet.add(ConfusionMatrix.NEGATIVE_CLASS)\n",
    "\n",
    "    sense_cm = ConfusionMatrix(sense_alphabet)\n",
    "    gold_to_predicted_map, predicted_to_gold_map = \\\n",
    "            _link_gold_predicted(gold_list, predicted_list, spans_exact_matching)\n",
    "\n",
    "    for i, gold_relation in enumerate(gold_list):\n",
    "        gold_sense = gold_relation['Sense'][0]\n",
    "        if gold_sense in valid_senses:\n",
    "            if i in gold_to_predicted_map:\n",
    "                predicted_sense = gold_to_predicted_map[i]['Sense'][0]\n",
    "                if predicted_sense in gold_relation['Sense']:\n",
    "                    sense_cm.add(predicted_sense, predicted_sense)\n",
    "                else:\n",
    "                    if not sense_cm.alphabet.has_label(predicted_sense):\n",
    "                        predicted_sense = ConfusionMatrix.NEGATIVE_CLASS\n",
    "                    sense_cm.add(predicted_sense, gold_sense)\n",
    "            else:\n",
    "                sense_cm.add(ConfusionMatrix.NEGATIVE_CLASS, gold_sense)\n",
    "\n",
    "    for i, predicted_relation in enumerate(predicted_list):\n",
    "        if i not in predicted_to_gold_map:\n",
    "            predicted_sense = predicted_relation['Sense'][0]\n",
    "            if not sense_cm.alphabet.has_label(predicted_sense):\n",
    "                predicted_sense = ConfusionMatrix.NEGATIVE_CLASS\n",
    "            sense_cm.add(predicted_sense, ConfusionMatrix.NEGATIVE_CLASS)\n",
    "    return sense_cm\n",
    "\n",
    "\n",
    "def combine_spans(span1, span2):\n",
    "    \"\"\"Merge two text span dictionaries\n",
    "\n",
    "    \"\"\"\n",
    "    new_span = {}\n",
    "    new_span['CharacterSpanList'] = span1['CharacterSpanList'] + span2['CharacterSpanList']\n",
    "    new_span['SpanList'] = span1['SpanList'] + span2['SpanList']\n",
    "    new_span['RawText'] = span1['RawText'] + span2['RawText']\n",
    "    new_span['TokenList'] = span1['TokenList'] + span2['TokenList']\n",
    "    return new_span\n",
    "\n",
    "def compute_binary_eval_metric(gold_list, predicted_list, matching_fn):\n",
    "    \"\"\"Compute binary evaluation metric\n",
    "\n",
    "    \"\"\"\n",
    "    binary_alphabet = Alphabet()\n",
    "    binary_alphabet.add('yes')\n",
    "    binary_alphabet.add('no')\n",
    "    cm = ConfusionMatrix(binary_alphabet)\n",
    "    matched_predicted = [False for x in predicted_list]\n",
    "    for gold_span in gold_list:\n",
    "        found_match = False\n",
    "        for i, predicted_span in enumerate(predicted_list):\n",
    "            if matching_fn(gold_span, predicted_span) and not matched_predicted[i]:\n",
    "                cm.add('yes', 'yes')\n",
    "                matched_predicted[i] = True\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            cm.add('no', 'yes')\n",
    "    # Predicted span that does not match with any\n",
    "    for matched in matched_predicted:\n",
    "        if not matched:\n",
    "            cm.add('yes', 'no')\n",
    "    return cm\n",
    "\n",
    "\n",
    "def _link_gold_predicted(gold_list, predicted_list, matching_fn):\n",
    "    \"\"\"Link gold standard relations to the predicted relations\n",
    "\n",
    "    A pair of relations are linked when the arg1 and the arg2 match exactly.\n",
    "    We do this because we want to evaluate sense classification later.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two dictionaries:\n",
    "        1) mapping from gold relation index to predicted relation index\n",
    "        2) mapping from predicted relation index to gold relation index\n",
    "    \"\"\"\n",
    "    gold_to_predicted_map = {}\n",
    "    predicted_to_gold_map = {}\n",
    "    gold_arg12_list = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList']))\n",
    "            for x in gold_list]\n",
    "    predicted_arg12_list = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList']))\n",
    "            for x in predicted_list]\n",
    "    for gi, gold_span in enumerate(gold_arg12_list):\n",
    "        for pi, predicted_span in enumerate(predicted_arg12_list):\n",
    "            if matching_fn(gold_span, predicted_span):\n",
    "                gold_to_predicted_map[gi] = predicted_list[pi]\n",
    "                predicted_to_gold_map[pi] = gold_list[gi]\n",
    "    return gold_to_predicted_map, predicted_to_gold_map\n",
    "\n",
    "\n",
    "def main(gold,predicted):\n",
    "    #parser = argparse.ArgumentParser(\n",
    "    #    description=\"Evaluate system's output against the gold standard\")\n",
    "    #parser.add_argument('gold', help='Gold standard file')\n",
    "    #parser.add_argument('predicted', help='System output file')\n",
    "    #args = parser.parse_args()\n",
    "    gold_list = [json.loads(x) for x in open(gold)]\n",
    "    predicted_list = [json.loads(x) for x in open(predicted)]\n",
    "    print '\\n================================================'\n",
    "    print 'Evaluation for all discourse relations'\n",
    "    evaluate(gold_list, predicted_list)\n",
    "\n",
    "    print '\\n================================================'\n",
    "    print 'Evaluation for explicit discourse relations only'\n",
    "    explicit_gold_list = [x for x in gold_list if x['Type'] == 'Explicit']\n",
    "    explicit_predicted_list = [x for x in predicted_list if x['Type'] == 'Explicit']\n",
    "    evaluate(explicit_gold_list, explicit_predicted_list)\n",
    "\n",
    "    print '\\n================================================'\n",
    "    print 'Evaluation for non-explicit discourse relations only (Implicit, EntRel, AltLex)'\n",
    "    non_explicit_gold_list = [x for x in gold_list if x['Type'] != 'Explicit']\n",
    "    non_explicit_predicted_list = [x for x in predicted_list if x['Type'] != 'Explicit']\n",
    "    evaluate(non_explicit_gold_list, non_explicit_predicted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality Evaluation\n",
    "\n",
    "3 Parts\n",
    "- Connectives\n",
    "- Arg1/Arg2 Span\n",
    "- Sense Validation\n",
    "\n",
    "only exact matching pairs are True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "\n",
    "## Create Unified Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Arg1': {u'CharacterSpanList': [[517, 564]],\n",
       "  u'RawText': u'to restrict the RTC to Treasury borrowings only',\n",
       "  u'TokenList': [85, 86, 87, 88, 89, 90, 91, 92]},\n",
       " u'Arg2': {u'CharacterSpanList': [[573, 629]],\n",
       "  u'RawText': u'the agency receives specific congressional authorization',\n",
       "  u'TokenList': [95, 96, 97, 98, 99, 100]},\n",
       " u'Connective': {u'CharacterSpanList': [[566, 572]],\n",
       "  u'RawText': u'unless',\n",
       "  u'TokenList': [94]},\n",
       " u'DocID': u'wsj_2200',\n",
       " u'ID': 35709,\n",
       " u'Sense': [u'Expansion.Alternative'],\n",
       " u'Type': u'Explicit'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in stepanov_predicted_list if i['Type'] == 'Explicit' and i['DocID'] == 'wsj_2200'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Arg1': {u'CharacterSpanList': [[517, 564]],\n",
       "  u'RawText': u'to restrict the RTC to Treasury borrowings only',\n",
       "  u'TokenList': [[517, 519, 85, 2, 3],\n",
       "   [520, 528, 86, 2, 4],\n",
       "   [529, 532, 87, 2, 5],\n",
       "   [533, 536, 88, 2, 6],\n",
       "   [537, 539, 89, 2, 7],\n",
       "   [540, 548, 90, 2, 8],\n",
       "   [549, 559, 91, 2, 9],\n",
       "   [560, 564, 92, 2, 10]]},\n",
       " u'Arg2': {u'CharacterSpanList': [[573, 629]],\n",
       "  u'RawText': u'the agency receives specific congressional authorization',\n",
       "  u'TokenList': [[573, 576, 95, 2, 13],\n",
       "   [577, 583, 96, 2, 14],\n",
       "   [584, 592, 97, 2, 15],\n",
       "   [593, 601, 98, 2, 16],\n",
       "   [602, 615, 99, 2, 17],\n",
       "   [616, 629, 100, 2, 18]]},\n",
       " u'Connective': {u'CharacterSpanList': [[566, 572]],\n",
       "  u'RawText': u'unless',\n",
       "  u'TokenList': [[566, 572, 94, 2, 12]]},\n",
       " u'DocID': u'wsj_2200',\n",
       " u'ID': 35709,\n",
       " u'Sense': [u'Expansion.Alternative'],\n",
       " u'Type': u'Explicit'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in gold_list if i['Type'] == 'Explicit' and i['DocID'] == 'wsj_2200'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Relation is defined by:\n",
    "\n",
    "- DocID\n",
    "- Arg1 TokenSpan\n",
    "- Arg2 TokenSpan\n",
    "- Type\n",
    "- Connective (Explicit, maybe Implicit)\n",
    "- Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_format(relation,gold=True):\n",
    "    conn = dict(relation[\"Connective\"])\n",
    "    if conn.has_key(\"CharacterSpanList\"):\n",
    "        del conn[\"CharacterSpanList\"]\n",
    "    if not conn.has_key(\"RawText\"):\n",
    "        conn[\"RawText\"] = \"\"\n",
    "    \n",
    "    dic = {\n",
    "        \"DocID\": relation[\"DocID\"],\n",
    "        \"RelID\": relation[\"ID\"],\n",
    "        \"Sense\": relation[\"Sense\"],\n",
    "        \"Type\" : relation[\"Type\"],\n",
    "        \"Connective\": conn   \n",
    "    }\n",
    "    \n",
    "    if gold:\n",
    "        dic[\"Arg1TokenList\"] = [token[2] for token in relation[\"Arg1\"][\"TokenList\"]]\n",
    "        dic[\"Arg2TokenList\"] = [token[2] for token in relation[\"Arg2\"][\"TokenList\"]]\n",
    "        dic[\"Parser\"] = \"Gold\"\n",
    "    else:\n",
    "        dic[\"Arg1TokenList\"] = relation[\"Arg1\"][\"TokenList\"]\n",
    "        dic[\"Arg2TokenList\"] = relation[\"Arg2\"][\"TokenList\"]\n",
    "        dic[\"Parser\"] = \"Pred\"\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_comparison_format_to_file(relations,file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(relations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comparison_format_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_gold_file_path = \"data/conll2016/en.dev/relations.json\"\n",
    "dev_oslopots_file_path = \"data/submissions/oslopots_dev/output/output.json\"\n",
    "dev_stepanov_file_path = \"data/submissions/stepanov_dev/output/output.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main()\n",
    "gold_list = [json.loads(x) for x in open(dev_gold_file_path)]\n",
    "oslopots_predicted_list = [json.loads(x) for x in open(dev_oslopots_file_path)]\n",
    "stepanov_predicted_list = [json.loads(x) for x in open(dev_stepanov_file_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [(\"oslopots\",oslopots_predicted_list),(\"stepanov\",stepanov_predicted_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_relations = [create_comparison_format(gold,True) for gold in gold_list]\n",
    "write_comparison_format_to_file(gold_relations,\"data/project_files/dev_gold_relations.json\")\n",
    "\n",
    "for name,prediction in predictions:\n",
    "    relations = [create_comparison_format(pred,False) for pred in prediction]\n",
    "    write_comparison_format_to_file(relations,\"data/project_files/dev_\"+name+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Relations together\n",
    "\n",
    "Some of the relations are completely equal (because the evaluation only allows exact matching spans), but some only have parts in common or are completely missed by the parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "oslopots_relations = read_comparison_format_from_file(\"data/project_files/dev_oslopots.json\")\n",
    "gold_relations = read_comparison_format_from_file(\"data/project_files/dev_gold_relations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_arg(gold_token,pred_token):\n",
    "    diff = len(\n",
    "        set(pred_token).intersection(set(gold_token))\n",
    "    )/len(\n",
    "        set(pred_token).union(set(gold_token)))\n",
    "    if diff != 1 and diff != 0:\n",
    "        print diff\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gold_pred_rel(gold_list,pred_list):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIDs = set([rel[\"DocID\"] for rel in gold_relations])\n",
    "mapping = []\n",
    "for docID in docIDs:\n",
    "    pred_doc_rel = [rel for rel in oslopots_relations if rel['DocID'] == docID]\n",
    "    gold_doc_rel = [rel for rel in gold_relations if rel['DocID'] == docID]\n",
    "\n",
    "    for gold_rel in gold_doc_rel:\n",
    "        gold_arg1 = gold_rel[\"Arg1TokenList\"]\n",
    "        gold_arg2 = gold_rel[\"Arg2TokenList\"]\n",
    "        gold_pred_map = []\n",
    "        for pred_rel in pred_doc_rel:\n",
    "            pred_arg1 = pred_rel[\"Arg1TokenList\"]\n",
    "            pred_arg2 = pred_rel[\"Arg2TokenList\"]\n",
    "            gold_pred_map += [(diff_arg(gold_arg1,pred_arg1)+diff_arg(gold_arg2,pred_arg2))/2]\n",
    "        best_match_index = np.argmax(gold_pred_map)\n",
    "        mapping += [\n",
    "            [gold_rel[\"RelID\"],\n",
    "            pred_doc_rel[best_match_index][\"RelID\"]]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_gold,found_pred = zip(*mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(found_gold)/len(gold_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The recognition of the right spans is most of the times right by the examinated parsers. Therefore, it doesn't help to explore span overlapping anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine each relation of Gold and all Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rel(gold_list,parser_pred):\n",
    "    combinations = {gold[\"RelID\"]:[gold] for gold in gold_list}\n",
    "    for name,prediction in parser_pred:\n",
    "        for pred in prediction:\n",
    "            pred[\"Parser\"] = name\n",
    "            combinations[pred[\"RelID\"]] += [pred]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_senses(parse_pairs):\n",
    "    pair_values = parse_pairs.values()\n",
    "    rows = []\n",
    "    for pair in pair_values:\n",
    "        gold_sense = pair[0][\"Sense\"]\n",
    "        correct_pred = []\n",
    "        for parse in pair[1:]:\n",
    "            correct_pred += [parse in gold_sense]\n",
    "        \n",
    "        rows += [correct_pred]\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "oslopots_relations = read_comparison_format_from_file(\"data/project_files/dev_oslopots.json\")\n",
    "stepanov_relations = read_comparison_format_from_file(\"data/project_files/dev_stepanov.json\")\n",
    "gold_relations = read_comparison_format_from_file(\"data/project_files/dev_gold_relations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_pred = [(\"oslopots\",oslopots_relations),(\"stepanov\",stepanov_relations)]\n",
    "parser_names = [name for name,pred in parser_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = combine_rel(gold_relations,parser_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senses = compare_senses(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Arg1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-3ea952c7b596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_sense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_relations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparser_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-225-8034d4aec89d>\u001b[0m in \u001b[0;36mevaluate_sense\u001b[0;34m(gold_list, predicted_list)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0msense_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense_alphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mgold_to_predicted_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_to_gold_map\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0m_link_gold_predicted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspans_exact_matching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_relation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-8034d4aec89d>\u001b[0m in \u001b[0;36m_link_gold_predicted\u001b[0;34m(gold_list, predicted_list, matching_fn)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mpredicted_to_gold_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     gold_arg12_list = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList']))\n\u001b[0;32m--> 219\u001b[0;31m             for x in gold_list]\n\u001b[0m\u001b[1;32m    220\u001b[0m     predicted_arg12_list = [(x['DocID'], (x['Arg1']['TokenList'], x['Arg2']['TokenList']))\n\u001b[1;32m    221\u001b[0m             for x in predicted_list]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Arg1'"
     ]
    }
   ],
   "source": [
    "evaluate_sense(gold_relations,parser_pred[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
